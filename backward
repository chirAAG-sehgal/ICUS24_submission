from ultralytics import YOLO
import cv2
import os
import numpy as np
import torch
import time

def backward_function(shelf,model):
    
    def conv_image_2(coloured_image_path,depth_image_path):
        depth_image = cv2.imread(depth_image_path)
        colored_image = cv2.imread(coloured_image_path)

        _, depth_image_thresholded = cv2.threshold(depth_image, 10, 255, cv2.THRESH_TOZERO)
        _, depth_image_thresholded = cv2.threshold(depth_image_thresholded, 140, 255, cv2.THRESH_TOZERO_INV)

        depth_image_thresholded_grayscale = cv2.cvtColor(depth_image_thresholded, cv2.COLOR_BGR2GRAY)

        mask = cv2.inRange(depth_image_thresholded_grayscale, 10,140)

        result_image = cv2.bitwise_and(colored_image, colored_image,mask=mask)
        return result_image

    def rotate(image, angle):
        rows, cols = image.shape[:2]
        rotation_matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), -angle, 1)
        rotated_image = cv2.warpAffine(image, rotation_matrix, (cols, rows))
        return rotated_image

    def oriented_and_subtracted(image_path,depth_path,coordinate_path):
        img_preprocessed = conv_image_2(image_path,depth_path)
            
        with open(coordinate_path, "r") as file:
            angle = float(file.readline())
            image_preprocessed_rotated = rotate(img_preprocessed, angle)
        return image_preprocessed_rotated

    start_time_load = time.time()

    # Replace 'your_custom_weights.pt' with the path to your custom weights file
    custom_weights_path = 'yolo8/runs/detect/train2_yolov8s_oreientedsubtracted/weights/best.pt'

    # Load the custom weights into the YOLO model
    model = YOLO(custom_weights_path)

    images_or_sub = []
    for number in range(15):
        image_path = f"images/{shelf}B/img_{shelf}B_{number}.jpg"
        depth_path = f"images/{shelf}B/depth_{shelf}B_{number}.jpg"
        coord_path = f"images/{shelf}B/coordinate_{shelf}B_{number}.txt"
        image_oriented_subtracted = oriented_and_subtracted(image_path, depth_path, coord_path)
        images_or_sub.append(image_oriented_subtracted)

    results = model(images_or_sub, imgsz=(480, 640), conf=0.3)
    #print(f"Total NUmber of results = {len(results)}")
    end_time_load = time.time()
    print(f"Time to Load the model = {end_time_load - start_time_load}")
    
    start_time_algo = time.time()

    plant_bed_counts = []

    for r in results:
        coordinates = r.boxes.xyxy
        conf = r.boxes.conf.unsqueeze(1)
        cls = r.boxes.cls.unsqueeze(1)
        concatenated_results = torch.cat((coordinates, conf, cls), dim=1)
        sorted_results = concatenated_results[torch.argsort(concatenated_results[:, 2], descending=True)]
        unique_classes, counts = torch.unique(sorted_results[:, 5], return_counts=True)

        # Calculate plant bed count
        class_counts_filtered = counts[unique_classes == 3]
        plant_bed_counts.append(class_counts_filtered.sum().item())
        print(f"Plant Bed backward= {class_counts_filtered.sum().item()}")
        
    # Determine the general value of plant beds
    general_plant_bed_count = np.median(plant_bed_counts)

    # Filter out tensors based on the plant bed count
    valid_tensor_indices = [i for i, count in enumerate(plant_bed_counts) if count == general_plant_bed_count]

    #print(f"Valid Tensor Indices: {valid_tensor_indices}")

    if not valid_tensor_indices:
        print("No valid tensors found.")
        return

    # Choose the middle tensor among valid tensors
    middle_index = len(valid_tensor_indices) // 2
    #print(f"Middle index = {middle_index}")
    chosen_tensor = results[valid_tensor_indices[middle_index]]
    
    coordinates = chosen_tensor.boxes.xyxy
    conf = chosen_tensor.boxes.conf.unsqueeze(1)
    cls = chosen_tensor.boxes.cls.unsqueeze(1)
    concatenated_results = torch.cat((coordinates, conf, cls), dim=1)
    sorted_results = concatenated_results[torch.argsort(concatenated_results[:, 2], descending=True)]
    #print(f"Sorted Results = {sorted_results}")

        
    unique_classes, counts = torch.unique(sorted_results[:, 5], return_counts=True)

    # Filter out the occurrences of class 3
    class_counts_filtered = counts[unique_classes != 3]
    unique_classes_filtered = unique_classes[unique_classes != 3]

    # Check if there are occurrences of the target class
    if unique_classes_filtered.numel() > 0:
        most_occuring_class = unique_classes_filtered[class_counts_filtered.argmax()].item()
        target_class = most_occuring_class
        
        reference_class = 3

        # Create a new tensor to store the results
        result_tensor = torch.zeros(0, 8)  # 8 columns for the concatenated result

        # Iterate through rows and perform calculations
        for i in range(sorted_results.shape[0]):
            row = sorted_results[i, :]
            # Check if the row belongs to the target class (fruit)
            if row[-1] == target_class:
                target_class_center_x = (row[0] + row[2]) / 2  # Calculate the x-coordinate of the center of the target class bounding box
                target_class_center_y = (row[1] + row[3]) / 2  # Calculate the y-coordinate of the center of the target class bounding box
                # Check the above rows until a plant bed class is found
                for j in range(i - 1, -1, -1):
                    above_row = sorted_results[j, :]
                    if above_row[-1] == reference_class:
                        # Calculate the relative position of the fruit with respect to the right and upper edges of the plant bed
                        relative_position_right = (above_row[2] - target_class_center_x) / (above_row[2] - above_row[0])
                        relative_position_upper = (target_class_center_y - above_row[1]) / (above_row[3] - above_row[1])

                        # Append the results to the new tensor
                        result_tensor = torch.cat([result_tensor, torch.tensor([[
                            row[0], row[1], row[2], row[3],  # x_min, y_min, x_max, y_max
                            row[4], row[5],  # conf, class
                            relative_position_right, relative_position_upper  # right edge, upper edge
                        ]])])
                        break
            elif row[-1] == reference_class:
                # For rows with class 3, add the row directly with right and upper edge set to 0
                result_tensor = torch.cat([result_tensor, torch.tensor([[
                    row[0], row[1], row[2], row[3],  # x_min, y_min, x_max, y_max
                    row[4], row[5],  # conf, class
                    0.0, 0.0  # right edge, upper edge (set to 0 for class 3)
                ]])])

        torch.set_printoptions(precision=4, sci_mode=False)
        print(f"Backward Result = {result_tensor}")
        return result_tensor

    else:
        print("No occurrences of the target class found.")
        result_tensor = sorted_results
        print(f"Backward Result = {result_tensor}")
        return result_tensor

